{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is a continuation of Continuous Control in the same folder\n",
    "\n",
    "The reason I have started a new file is because I wanted to keep the old file as evidence of the work done to get to this point. Otherwise it is the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.19 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environments corresponding to both versions of the environment are already saved in the Workspace and can be accessed at the file paths provided below.  \n",
    "\n",
    "Please select one of the two options below for loading the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# select this option to load version 1 (with a single agent) of the environment\n",
    "env = UnityEnvironment(file_name='/data/Reacher_One_Linux_NoVis/Reacher_One_Linux_NoVis.x86_64')\n",
    "\n",
    "# select this option to load version 2 (with 20 agents) of the environment\n",
    "# env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726671e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "print(type(states[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment. But note - don't do this if you plan to train the agent. I cannot find a way of re-starting it, other than completely refreshing this notebook. Broken pipe etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See comments above - don't close the environment. You can always just reset it!\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- To structure your work, you're welcome to work directly in this Jupyter notebook, or you might like to start over with a new file!  You can see the list of files in the workspace by clicking on **_Jupyter_** in the top left corner of the notebook.\n",
    "- In this coding environment, you will not be able to watch the agents while they are training.  However, **_after training the agents_**, you can download the saved model weights to watch the agents on your own machine! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proposed approach by DCFW\n",
    "1. Use a DDPG actor-critic model to train the two agents (actor and critic).\n",
    "1. Use the Udacity DDPG - pendulum code as a starter code to work with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first import torch modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also import other required modules\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define helper function for initialising layers.\n",
    "# Note, pytorch does this automatically for the first intialisation.\n",
    "# Note, I believe Udacity code for DDPG pendulum was slightly wrong - used the output features as the basis,\n",
    "# not the input features. To check out.\n",
    "def hidden_init(layer):\n",
    "    \"\"\"Helper function. Just returns the limits used for weights initialisation in the hidden layer\n",
    "    \"\"\"\n",
    "    f_out, f_in = layer.weight.data.size()\n",
    "    lim = 1. / np.sqrt(f_in)\n",
    "    return (-lim, lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Actor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Actor. This is the agent that is going to try to learn the best policy\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed) # for reproducibility\n",
    "        self.fc1 = nn.Linear(in_features=state_size, out_features=fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"reset the weighs of the layer to random uniform distribution (range = sqrt of number of input features)\"\"\"\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3) # find out why hard-coded? Same for Critic \n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x)) # INVESTIGATE this - whether better activation fn would be useful. tanh\n",
    "                                    # will map to between -1 and +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate / test some of the features of the Actor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actor summary:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=33, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "Summary of second hidden layer\n",
      "Linear(in_features=256, out_features=128, bias=True)\n",
      "\n",
      "Torch tensor of the dimensions of second hidden layer:\n",
      "torch.Size([128, 256])\n",
      "\n",
      "Torch initialises weights uniform random between +- sqrt(number of features in)\n",
      "\n",
      "To check whether this is true, the min and max of second hidden layer are:\n",
      "-0.0624967 0.0624897\n",
      "... and the sqrt of number of input features to second hidden layer of 256 is:\n",
      "0.0625\n",
      "\n",
      "Created a random state of the following type (should be numpy array, like the Unity environment will provide)\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "Output from one forward pass of the model - should be tensor of size 4, corresponding to the 4 actions\n",
      "tensor(1.00000e-02 *\n",
      "       [ 5.2321, -7.0089, -2.3594, -1.1062])\n"
     ]
    }
   ],
   "source": [
    "# Demo some of the features of the Actor model\n",
    "actor = Actor(state_size=33, action_size=4, seed=42)\n",
    "print ('Actor summary:')\n",
    "print (actor)\n",
    "print ('\\nSummary of second hidden layer')\n",
    "print (actor.fc2)\n",
    "print ('\\nTorch tensor of the dimensions of second hidden layer:')\n",
    "print (actor.fc2.weight.data.size())\n",
    "print ('\\nTorch initialises weights uniform random between +- sqrt(number of features in)')\n",
    "print ('\\nTo check whether this is true, the min and max of second hidden layer are:')\n",
    "print (actor.fc2.weight.data.numpy().min(), actor.fc2.weight.data.numpy().max())\n",
    "print ('... and the sqrt of number of input features to second hidden layer of {} is:'\n",
    "       .format(actor.fc2.weight.data.size()[1]))\n",
    "print (1. / np.sqrt(actor.fc2.weight.data.size()[1]))\n",
    "\n",
    "state = np.array([np.random.random() for i in range(33)])\n",
    "print ('\\nCreated a random state of the following type (should be numpy array, like the Unity environment will provide)')\n",
    "print (type(state))\n",
    "print ('\\nOutput from one forward pass of the model - should be tensor of size 4, corresponding to the 4 actions')\n",
    "print (actor.forward(torch.from_numpy(state).float()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the Critic model\n",
    "The critic is trying to learn the value-action of the state environment rather than the policy.  \n",
    "This will help speed up the learning rather than trying to learn the policy just on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=256, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(in_features=state_size, out_features=fcs1_units)\n",
    "        # note the difference in output units to input units - we are going to concatenate the actions here\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"reset the weighs of the layer to random uniform distribution (range = sqrt of number of input features)\"\"\"\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.relu(self.fcs1(state))\n",
    "        # we now need to add in the actions - why do we not do this at the first layer?\n",
    "        t_xs = torch.transpose(xs, 0, 1)\n",
    "        t_action = torch.transpose(action, 0, 1)\n",
    "        x = torch.transpose(torch.cat((t_xs, t_action)), 0,1) # check whether I need to put in the dimension?\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # no need to put through an activation layer, as we want the VALUE of the state-action\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critic(\n",
      "  (fcs1): Linear(in_features=33, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=260, out_features=128, bias=True)\n",
      "  (fc3): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Examine the Critic model - no need to do the full exploration as for the Actor\n",
    "c = Critic(state_size=33, action_size=4, seed=42)\n",
    "print (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1])\n"
     ]
    }
   ],
   "source": [
    "# create some dummy states and actions to check the forward pass\n",
    "state = np.random.random((128,33))\n",
    "state = torch.from_numpy(state).float()\n",
    "\n",
    "actions = np.random.random((128,4))\n",
    "actions = torch.from_numpy(actions).float()\n",
    "\n",
    "# check the forward pass. This should create a scalar value for the state action pair.\n",
    "out =c.forward(state=state, action=actions)\n",
    "print (out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the device being used. We need to do this in the set up of the main Agent\n",
    "NOTE - set up and test the model using CPU first, then run training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device has been set, it is: cuda:0\n",
      "We are using GPU for TRAINING\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('device has been set, it is: {}'.format(device))\n",
    "if device.type == 'cuda':\n",
    "    print('We are using GPU for TRAINING')\n",
    "else:\n",
    "    print('We are using CPU to set up and test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the DEFAULT hyper-parameters\n",
    "These have been taken from the [Udacity Deep Reinforcement Learning DDPG-pendulum code](ahttps://github.com/udacity/deep-reinforcement-learning), as a starting point to explore the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size, Udacity_pendulum = 1e5\n",
    "BATCH_SIZE = 128        # minibatch size, Udacity_pendulum = 128\n",
    "GAMMA = 0.99            # discount factor, Udacity_pendulum = 0.99\n",
    "TAU = 1e-3              # for soft update of target parameters, Udacity_pendulum = 1e-3\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor , Udacity_pendulum = 1e-4\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic, Udacity_pendulum = 1e-3\n",
    "WEIGHT_DECAY = 0        # L2 weight decay, Udacity_pendulum = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define OUNoise which is used to add a bit of noise if you want a little\n",
    "# more exploratory action - you add it to the calculated action vector\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process\"\"\"\n",
    "    \n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (=noise) to mean (mu)\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "        \n",
    "    def sample(self, epsilon=1.0):\n",
    "        \"\"\"Update internal state and return it as a noise sample\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + epsilon* self.sigma * np.array(\n",
    "#             [random.random() for i in range(len(x))])\n",
    "            [random.random() -0.5 for i in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: [ 0.  0.  0.  0.  0.]\n",
      "After: [ 0.00811521 -0.03052455  0.04652511  0.04239764 -0.00328613]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate the funcition of OUNoise\n",
    "oun = OUNoise(size=(5), seed=101)\n",
    "print ('Before: {}'.format(oun.state))\n",
    "print ('After: {}'.format(oun.sample(epsilon=0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the Replay Buffer - re-using paths multiple times\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed size buffer to store experience tuples\"\"\"\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen= buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\n",
    "            \"state\", \"action\", \"reward\", \"next_state\", \"done\" \n",
    "        ])\n",
    "        self.seed = random.seed()\n",
    "        \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experinece to the ReplayBuffer memory\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences\n",
    "                    if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences\n",
    "                    if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences \n",
    "                    if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences \n",
    "                    if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences \n",
    "                    if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def len(self):\n",
    "        \"\"\"Return the current size of the internal memory\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts and learns from the environment\n",
    "    Uses the Actor and Critic in tandem\"\"\"\n",
    "    \n",
    "    def __init__(self, hypers):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "    \n",
    "        self.state_size = hypers['state_size']\n",
    "        self.action_size = hypers['action_size']\n",
    "        self.seed = hypers['random_seed']\n",
    "        self.actor_fc1_units = hypers['actor_fc1_units']\n",
    "        self.actor_fc2_units = hypers['actor_fc2_units']\n",
    "        self.lr_actor = hypers['lr_actor']\n",
    "        self.critic_fcs1_units = hypers['critic_fcs1_units']\n",
    "        self.critic_fc2_units = hypers['critic_fc2_units']\n",
    "        self.lr_critic = hypers['lr_critic']\n",
    "        self.weight_decay = hypers['weight_decay']\n",
    "        self.learn_every = hypers['learn_every']\n",
    "        self.oun_mu = hypers['oun_mu']\n",
    "        self.oun_theta = hypers['oun_theta']\n",
    "        self.oun_sigma = hypers['oun_sigma']\n",
    "        self.buffer_size = hypers['buffer_size']\n",
    "        self.batch_size = hypers['batch_size']\n",
    "        self.gamma = hypers['gamma']\n",
    "        self.tau = hypers['tau']\n",
    "        self.learn_counter = 0\n",
    "        \n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(self.state_size, self.action_size, self.seed, \n",
    "                                 self.actor_fc1_units, self.actor_fc2_units ).to(device)\n",
    "        self.actor_target = Actor(self.state_size, self.action_size, self.seed,\n",
    "                                 self.actor_fc1_units, self.actor_fc2_units).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(),\n",
    "                                lr=self.lr_actor)\n",
    "        \n",
    "        # Critic Network (w/Target Network)\n",
    "        self.critic_local = Critic(self.state_size, self.action_size, self.seed,\n",
    "                                  self.critic_fcs1_units, self.critic_fc2_units).to(device)\n",
    "        self.critic_target = Critic(self.state_size, self.action_size, self.seed,\n",
    "                                   self.critic_fcs1_units, self.critic_fc2_units).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(),\n",
    "                                lr=self.lr_critic, weight_decay= self.weight_decay)\n",
    "        \n",
    "        # Noise process\n",
    "        self.noise = OUNoise(self.action_size, self.seed, self.oun_mu, self.oun_theta, self.oun_sigma)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(self.action_size, self.buffer_size, self.batch_size,\n",
    "                                  self.seed)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save the experience in the reply memory, and use random\n",
    "        sample from buffer to learn\"\"\"\n",
    "        # save the experience /reward etc\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # check there are enough samples in memory, and then learn\n",
    "        if self.memory.len() > self.batch_size and self.learn_counter % self.learn_every ==0:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, self.gamma)\n",
    "        self.learn_counter +=1\n",
    "    \n",
    "    def act(self, state, add_noise=True, epsilon=1.0):\n",
    "        \"\"\"Returns actions for a given state as per current policy\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        # forward pass through the actor neural net to give actions\n",
    "        self.actor_local.eval()\n",
    "        # turn off autograd for this one step as it speeds this up (and we don't need it here)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample(epsilon)\n",
    "        return np.clip(action, -1, 1) # it is bounded to +/-1\n",
    "    \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "    \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # -------------------update critic-----------------------#\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1- dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimise the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # -------------------update actor ------------------------#\n",
    "        # compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        # remember - we are doing gradient ASCENT for actor policy\n",
    "        # so turn it negative and run gradient descent....\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss (=maximise the gain in grad ascent)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # -------------------update target networks --------------#\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(\n",
    "            target_model.parameters(), local_model.parameters()):\n",
    "                target_param.data.copy_(tau*local_param.data +\n",
    "                    (1.0-tau) * target_param.data)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to set up the hyper-parameters, because we pass these into the agent at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_hyperparameters(**kwargs):\n",
    "    \"\"\"return a hyper-parameter dictionary to use in training\"\"\"\n",
    "    # populate the dictionary first with the default ones. We will over-write them as we tweak the training.\n",
    "    hypers = {\n",
    "        'state_size': 33,\n",
    "        'action_size': 4,\n",
    "        'random_seed': 101,\n",
    "        'buffer_size': BUFFER_SIZE,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'gamma': GAMMA,\n",
    "        'tau': TAU,\n",
    "        'lr_actor': LR_ACTOR,\n",
    "        'lr_critic': LR_CRITIC,\n",
    "        'weight_decay': WEIGHT_DECAY,\n",
    "        'learn_every': 5,\n",
    "        # now we are onto dcfw 'defaults',\n",
    "        'n_episodes': 300, # number of episodes - set at 300 to see early results, otherwise need c. 1000\n",
    "        'max_t': 1000, # this needs to be 1,000, or otherwise set the loop to while True.\n",
    "        'print_every': 50,\n",
    "        'oun_mu': 0.,\n",
    "        'oun_theta': 0.15,\n",
    "        'oun_sigma': 0.2,\n",
    "        'actor_fc1_units': 256,\n",
    "        'actor_fc2_units': 128,\n",
    "        'critic_fcs1_units': 256,\n",
    "        'critic_fc2_units': 128,\n",
    "        'save_every': 20,\n",
    "        'save_name': '_default',\n",
    "        'save_data_name': '_data_default',\n",
    "        'load_from_actor': '',\n",
    "        'load_from_critic': ''\n",
    "        }\n",
    "    \n",
    "    for key, value in kwargs.items():\n",
    "        hypers[key] = value\n",
    "    \n",
    "    return hypers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(hypers):\n",
    "    print ('Hyper-parameters used')\n",
    "    for key, value in hypers.items():\n",
    "        if len(key)> 14:\n",
    "            print(key,\"\\t\", value)\n",
    "        elif len(key) >6:\n",
    "            print (key,\"\\t\\t\", value)\n",
    "        else:\n",
    "            print(key, \"\\t\\t\\t\",value)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 16\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0001\n",
      "lr_critic \t\t 0.001\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 50\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t _default\n",
      "save_data_name \t\t _data_default\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, batch_size=16)\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we have the actor and critic set up, as well as the Agent which combines the two when interacting with the enviroment.\n",
    "Run the folllowing cell, to check the environment is still operating, if not run the cells at the start of the workbook again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.0001\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent = Agent(hypers=hypers)\n",
    "agent.actor_optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(hypers):\n",
    "    \"\"\"Run the DDPG agent through episodes, using Actor and Critic\"\"\"\n",
    "    \n",
    "    agent = Agent(hypers=hypers)\n",
    "    if hypers['load_from_actor']:\n",
    "        agent.actor_local.load_state_dict(hypers['load_from_actor'])\n",
    "        \n",
    "    if hypers['load_from_critic']:\n",
    "        agent.critic_local.load_state_dict(hypers['load_from_critic'])\n",
    "    \n",
    "    n_episodes = hypers['n_episodes']\n",
    "    max_t = hypers['max_t']\n",
    "    print_every = hypers['print_every']\n",
    "    save_every = hypers['save_every']\n",
    "    check_point_actor = 'checkpoint_actor_' + hypers['save_name']+'_'\n",
    "    check_point_critic = 'checkpoint_critic_' + hypers['save_name']+'_'\n",
    "    scores_deque = deque(maxlen = 100) # this is the test we are using - have to get avg > 30 over 100 episodes\n",
    "    scores=[]\n",
    "    epsilon = 1\n",
    "    for i_episode in range(1, n_episodes +1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state =env_info.vector_observations\n",
    "        agent.reset() # note - this resets the noise only!\n",
    "#         epsilon *= EPSILON_RATE\n",
    "        \n",
    "        score=0\n",
    "        while True:\n",
    "#         for t in range(max_t):\n",
    "            action = agent.act(state, epsilon)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward[0]\n",
    "            if done[0]:\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {} of {}\\tAvg Score: {:.2f}'.format(i_episode, n_episodes, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % save_every == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), check_point_actor + str(i_episode))\n",
    "            torch.save(agent.critic_local.state_dict(), check_point_critic + str(i_episode))\n",
    "            with open(hypers['save_data_name']+'.pkl', 'wb') as file:\n",
    "                pickle.dump(scores, file)\n",
    "        if i_episode % print_every == 0:\n",
    "                print('\\rEpisode {}\\tAvg Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) > 30:\n",
    "            print ('Task completed on episode {}'.format(i_episode))\n",
    "            torch.save(agent.actor_local.state_dict(), check_point_actor + '_SOLVED')\n",
    "            torch.save(agent.critic_local.state_dict(), check_point_critic + '_SOLVED')\n",
    "            with open(hypers['save_data_name']+'.pkl', 'wb') as file:\n",
    "                pickle.dump(scores, file)\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=10, save_name='test', print_every = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5\tAvg Score: 0.89: 0.89\n",
      "Episode 10\tAvg Score: 0.47: 0.47\n"
     ]
    }
   ],
   "source": [
    "scores = ddpg(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fddb84b0358>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's try running that for a bit longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, save_name='test', print_every = 5, actor_fc1_units=128,\n",
    "                               critic_fcs1_units=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0001\n",
      "lr_critic \t\t 0.001\n",
      "weight_decay \t\t 0\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 5\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 128\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 128\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t test\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from workspace_utils import active_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Episode 1 of 300\tAvg Score: 0.002\n",
      "Episode 2 of 300\tAvg Score: 0.363\n",
      "Episode 3 of 300\tAvg Score: 0.354\n",
      "Episode 4 of 300\tAvg Score: 0.525\n",
      "Episode 5\tAvg Score: 0.74e: 0.74\n",
      "6\n",
      "Episode 6 of 300\tAvg Score: 0.737\n",
      "Episode 7 of 300\tAvg Score: 0.708\n",
      "Episode 8 of 300\tAvg Score: 0.689\n",
      "Episode 9 of 300\tAvg Score: 0.6910\n",
      "Episode 10\tAvg Score: 0.66e: 0.66\n",
      "11\n",
      "Episode 11 of 300\tAvg Score: 0.6012\n",
      "Episode 12 of 300\tAvg Score: 0.5513\n",
      "Episode 13 of 300\tAvg Score: 0.5314\n",
      "Episode 14 of 300\tAvg Score: 0.5615\n",
      "Episode 15\tAvg Score: 0.56e: 0.56\n",
      "16\n",
      "Episode 16 of 300\tAvg Score: 0.5817\n",
      "Episode 17 of 300\tAvg Score: 0.6218\n",
      "Episode 18 of 300\tAvg Score: 0.6619\n",
      "Episode 19 of 300\tAvg Score: 0.7120\n",
      "Episode 20\tAvg Score: 0.75e: 0.75\n",
      "21\n",
      "Episode 21 of 300\tAvg Score: 0.8122\n",
      "Episode 22 of 300\tAvg Score: 0.7823\n",
      "Episode 23 of 300\tAvg Score: 0.8124\n",
      "Episode 24 of 300\tAvg Score: 0.8125\n",
      "Episode 25\tAvg Score: 0.81e: 0.81\n",
      "26\n",
      "Episode 26 of 300\tAvg Score: 0.8327\n",
      "Episode 27 of 300\tAvg Score: 0.8528\n",
      "Episode 28 of 300\tAvg Score: 0.8329\n",
      "Episode 29 of 300\tAvg Score: 0.8330\n",
      "Episode 30\tAvg Score: 0.80e: 0.80\n",
      "31\n",
      "Episode 31 of 300\tAvg Score: 0.7832\n",
      "Episode 32 of 300\tAvg Score: 0.7733\n",
      "Episode 33 of 300\tAvg Score: 0.8234\n",
      "Episode 34 of 300\tAvg Score: 0.8135\n",
      "Episode 35\tAvg Score: 0.83e: 0.83\n",
      "36\n",
      "Episode 36 of 300\tAvg Score: 0.8437\n",
      "Episode 37 of 300\tAvg Score: 0.8538\n",
      "Episode 38 of 300\tAvg Score: 0.8839\n",
      "Episode 39 of 300\tAvg Score: 0.9040\n",
      "Episode 40\tAvg Score: 0.90e: 0.90\n",
      "41\n",
      "Episode 41 of 300\tAvg Score: 0.9042\n",
      "Episode 42 of 300\tAvg Score: 0.9143\n",
      "Episode 43 of 300\tAvg Score: 0.9144\n",
      "Episode 44 of 300\tAvg Score: 0.9345\n",
      "Episode 45\tAvg Score: 0.96e: 0.96\n",
      "46\n",
      "Episode 46 of 300\tAvg Score: 1.0047\n",
      "Episode 47 of 300\tAvg Score: 1.0448\n",
      "Episode 48 of 300\tAvg Score: 1.0649\n",
      "Episode 49 of 300\tAvg Score: 1.0850\n",
      "Episode 50\tAvg Score: 1.09e: 1.09\n",
      "51\n",
      "Episode 51 of 300\tAvg Score: 1.1152\n",
      "Episode 52 of 300\tAvg Score: 1.1153\n",
      "Episode 53 of 300\tAvg Score: 1.1154\n",
      "Episode 54 of 300\tAvg Score: 1.1355\n",
      "Episode 55\tAvg Score: 1.12e: 1.12\n",
      "56\n",
      "Episode 56 of 300\tAvg Score: 1.1257\n",
      "Episode 57 of 300\tAvg Score: 1.1158\n",
      "Episode 58 of 300\tAvg Score: 1.1059\n",
      "Episode 59 of 300\tAvg Score: 1.1060\n",
      "Episode 60\tAvg Score: 1.11e: 1.11\n",
      "61\n",
      "Episode 61 of 300\tAvg Score: 1.1262\n",
      "Episode 62 of 300\tAvg Score: 1.1263\n",
      "Episode 63 of 300\tAvg Score: 1.1364\n",
      "Episode 64 of 300\tAvg Score: 1.1365\n",
      "Episode 65\tAvg Score: 1.12e: 1.12\n",
      "66\n",
      "Episode 66 of 300\tAvg Score: 1.1267\n",
      "Episode 67 of 300\tAvg Score: 1.1168\n",
      "Episode 68 of 300\tAvg Score: 1.1169\n",
      "Episode 69 of 300\tAvg Score: 1.1270\n",
      "Episode 70\tAvg Score: 1.12e: 1.12\n",
      "71\n",
      "Episode 71 of 300\tAvg Score: 1.1272\n",
      "Episode 72 of 300\tAvg Score: 1.1273\n",
      "Episode 73 of 300\tAvg Score: 1.1274\n",
      "Episode 74 of 300\tAvg Score: 1.1275\n",
      "Episode 75\tAvg Score: 1.11e: 1.11\n",
      "76\n",
      "Episode 76 of 300\tAvg Score: 1.1177\n",
      "Episode 77 of 300\tAvg Score: 1.1178\n",
      "Episode 78 of 300\tAvg Score: 1.1079\n",
      "Episode 79 of 300\tAvg Score: 1.0980\n",
      "Episode 80\tAvg Score: 1.09e: 1.09\n",
      "81\n",
      "Episode 81 of 300\tAvg Score: 1.1082\n",
      "Episode 82 of 300\tAvg Score: 1.0983\n",
      "Episode 83 of 300\tAvg Score: 1.0984\n",
      "Episode 84 of 300\tAvg Score: 1.0985\n",
      "Episode 85\tAvg Score: 1.09e: 1.09\n",
      "86\n",
      "Episode 86 of 300\tAvg Score: 1.0987\n",
      "Episode 87 of 300\tAvg Score: 1.0988\n",
      "Episode 88 of 300\tAvg Score: 1.0989\n",
      "Episode 89 of 300\tAvg Score: 1.0890\n",
      "Episode 90\tAvg Score: 1.08e: 1.08\n",
      "91\n",
      "Episode 91 of 300\tAvg Score: 1.0892\n",
      "Episode 92 of 300\tAvg Score: 1.0893\n",
      "Episode 93 of 300\tAvg Score: 1.0894\n",
      "Episode 94 of 300\tAvg Score: 1.0895\n",
      "Episode 95\tAvg Score: 1.07e: 1.07\n",
      "96\n",
      "Episode 96 of 300\tAvg Score: 1.0797\n",
      "Episode 97 of 300\tAvg Score: 1.0698\n",
      "Episode 98 of 300\tAvg Score: 1.0699\n",
      "Episode 99 of 300\tAvg Score: 1.06100\n",
      "Episode 100\tAvg Score: 1.06e: 1.06\n",
      "101\n",
      "Episode 101 of 300\tAvg Score: 1.07102\n",
      "Episode 102 of 300\tAvg Score: 1.07103\n",
      "Episode 103 of 300\tAvg Score: 1.08104\n",
      "Episode 104 of 300\tAvg Score: 1.07105\n",
      "Episode 105\tAvg Score: 1.07e: 1.07\n",
      "106\n",
      "Episode 106 of 300\tAvg Score: 1.08107\n",
      "Episode 107 of 300\tAvg Score: 1.08108\n",
      "Episode 108 of 300\tAvg Score: 1.08109\n",
      "Episode 109 of 300\tAvg Score: 1.09110\n",
      "Episode 110\tAvg Score: 1.09e: 1.09\n",
      "111\n",
      "Episode 111 of 300\tAvg Score: 1.10112\n",
      "Episode 112 of 300\tAvg Score: 1.11113\n",
      "Episode 113 of 300\tAvg Score: 1.12114\n",
      "Episode 114 of 300\tAvg Score: 1.12115\n",
      "Episode 115\tAvg Score: 1.12e: 1.12\n",
      "116\n",
      "Episode 116 of 300\tAvg Score: 1.12117\n",
      "Episode 117 of 300\tAvg Score: 1.15118\n",
      "Episode 118 of 300\tAvg Score: 1.14119\n",
      "Episode 119 of 300\tAvg Score: 1.15120\n",
      "Episode 120\tAvg Score: 1.14e: 1.14\n",
      "121\n",
      "Episode 121 of 300\tAvg Score: 1.13122\n",
      "Episode 122 of 300\tAvg Score: 1.14123\n",
      "Episode 123 of 300\tAvg Score: 1.14124\n",
      "Episode 124 of 300\tAvg Score: 1.15125\n",
      "Episode 125\tAvg Score: 1.15e: 1.15\n",
      "126\n",
      "Episode 126 of 300\tAvg Score: 1.15127\n",
      "Episode 127 of 300\tAvg Score: 1.15128\n",
      "Episode 128 of 300\tAvg Score: 1.16129\n",
      "Episode 129 of 300\tAvg Score: 1.16130\n",
      "Episode 130\tAvg Score: 1.17e: 1.17\n",
      "131\n",
      "Episode 131 of 300\tAvg Score: 1.18132\n",
      "Episode 132 of 300\tAvg Score: 1.19133\n",
      "Episode 133 of 300\tAvg Score: 1.18134\n",
      "Episode 134 of 300\tAvg Score: 1.19135\n",
      "Episode 135\tAvg Score: 1.19e: 1.19\n",
      "136\n",
      "Episode 136 of 300\tAvg Score: 1.20137\n",
      "Episode 137 of 300\tAvg Score: 1.21138\n",
      "Episode 138 of 300\tAvg Score: 1.21139\n",
      "Episode 139 of 300\tAvg Score: 1.20140\n",
      "Episode 140\tAvg Score: 1.21e: 1.21\n",
      "141\n",
      "Episode 141 of 300\tAvg Score: 1.22142\n",
      "Episode 142 of 300\tAvg Score: 1.21143\n",
      "Episode 143 of 300\tAvg Score: 1.21144\n",
      "Episode 144 of 300\tAvg Score: 1.20145\n",
      "Episode 145\tAvg Score: 1.21e: 1.21\n",
      "146\n",
      "Episode 146 of 300\tAvg Score: 1.20147\n",
      "Episode 147 of 300\tAvg Score: 1.19148\n",
      "Episode 148 of 300\tAvg Score: 1.18149\n",
      "Episode 149 of 300\tAvg Score: 1.18150\n",
      "Episode 150\tAvg Score: 1.19e: 1.19\n",
      "151\n",
      "Episode 151 of 300\tAvg Score: 1.18152\n",
      "Episode 152 of 300\tAvg Score: 1.19153\n",
      "Episode 153 of 300\tAvg Score: 1.19154\n",
      "Episode 154 of 300\tAvg Score: 1.19155\n",
      "Episode 155\tAvg Score: 1.20e: 1.20\n",
      "156\n",
      "Episode 156 of 300\tAvg Score: 1.19157\n",
      "Episode 157 of 300\tAvg Score: 1.20158\n",
      "Episode 158 of 300\tAvg Score: 1.19159\n",
      "Episode 159 of 300\tAvg Score: 1.20160\n",
      "Episode 160\tAvg Score: 1.20e: 1.20\n",
      "161\n",
      "Episode 161 of 300\tAvg Score: 1.21162\n",
      "Episode 162 of 300\tAvg Score: 1.21163\n",
      "Episode 163 of 300\tAvg Score: 1.20164\n",
      "Episode 164 of 300\tAvg Score: 1.21165\n",
      "Episode 165\tAvg Score: 1.23e: 1.23\n",
      "166\n",
      "Episode 166 of 300\tAvg Score: 1.22167\n",
      "Episode 167 of 300\tAvg Score: 1.24168\n",
      "Episode 168 of 300\tAvg Score: 1.23169\n",
      "Episode 169 of 300\tAvg Score: 1.22"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a72f977eb422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mactive_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mddpg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhypers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-e264218eaf4e>\u001b[0m in \u001b[0;36mddpg\u001b[0;34m(hypers)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-0d9d9539aa90>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-0d9d9539aa90>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# Minimize the loss (=maximise the gain in grad ascent)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mactor_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, save_name='run2', print_every = 10, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 256\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0001\n",
      "lr_critic \t\t 0.001\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 10\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run2\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I actually did run with those parameters for 300 episodes, but didn't save the print-outs. It was very slow learning - getting to only 1.45 average after 300 esisodes. Therefore will try to increase the learning rate for both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, save_name='run3', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 1e-3, lr_critic = 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.001\n",
      "lr_critic \t\t 0.005\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run3\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.57e: 0.57\n",
      "Episode 40\tAvg Score: 0.36e: 0.36\n",
      "Episode 60\tAvg Score: 0.25e: 0.25\n",
      "Episode 80\tAvg Score: 0.19e: 0.19\n",
      "Episode 100\tAvg Score: 0.15e: 0.15\n",
      "Episode 120\tAvg Score: 0.04e: 0.04\n",
      "Episode 140\tAvg Score: 0.02e: 0.02\n",
      "Episode 160\tAvg Score: 0.01e: 0.01\n",
      "Episode 180\tAvg Score: 0.01e: 0.01\n",
      "Episode 200\tAvg Score: 0.01e: 0.01\n",
      "Episode 220\tAvg Score: 0.01e: 0.01\n",
      "Episode 240\tAvg Score: 0.01e: 0.01\n",
      "Episode 260\tAvg Score: 0.01e: 0.01\n",
      "Episode 280\tAvg Score: 0.02e: 0.02\n",
      "Episode 300\tAvg Score: 0.02e: 0.02\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well. That was pretty disastrous!   \n",
    "Next effort - move the learning rates back down to, say 5e-4 each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0005\n",
      "lr_critic \t\t 0.0005\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run4\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, save_name='run4', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 5e-4, lr_critic = 5e-4)\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.91e: 0.91\n",
      "Episode 40\tAvg Score: 0.78e: 0.78\n",
      "Episode 60\tAvg Score: 0.58e: 0.58\n",
      "Episode 80\tAvg Score: 0.63e: 0.63\n",
      "Episode 100\tAvg Score: 0.78e: 0.78\n",
      "Episode 120\tAvg Score: 1.07e: 1.07\n",
      "Episode 140\tAvg Score: 1.50e: 1.50\n",
      "Episode 160\tAvg Score: 2.06e: 2.06\n",
      "Episode 180\tAvg Score: 2.57e: 2.57\n",
      "Episode 200\tAvg Score: 3.34e: 3.34\n",
      "Episode 220\tAvg Score: 4.21e: 4.21\n",
      "Episode 240\tAvg Score: 5.24e: 5.24\n",
      "Episode 260\tAvg Score: 5.93e: 5.93\n",
      "Episode 280\tAvg Score: 7.10e: 7.10\n",
      "Episode 300\tAvg Score: 7.68e: 7.68\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so at this point, I returned to Udacity website - the session had closed itself down. But the checkpoints had worked, and the weights saved..... but annoyingly I called them the same name, so the critic weights over-rode the actor weights. As it looked like there was some promise to training, I'll fix the code, so that the critic weights are saved also. But I will run the training with more episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0005\n",
      "lr_critic \t\t 0.0005\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 1000\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.15\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run4\n",
      "save_data_name \t\t _data_default\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=1000, save_name='run4', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 5e-4, lr_critic = 5e-4)\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.41re: 0.41\n",
      "Episode 40\tAvg Score: 0.68re: 0.68\n",
      "Episode 60\tAvg Score: 0.64re: 0.64\n",
      "Episode 80\tAvg Score: 0.74re: 0.74\n",
      "Episode 100\tAvg Score: 0.95re: 0.95\n",
      "Episode 120\tAvg Score: 1.30re: 1.30\n",
      "Episode 140\tAvg Score: 1.66re: 1.66\n",
      "Episode 160\tAvg Score: 2.02re: 2.02\n",
      "Episode 180\tAvg Score: 2.46re: 2.46\n",
      "Episode 200\tAvg Score: 2.81re: 2.81\n",
      "Episode 220\tAvg Score: 3.20re: 3.20\n",
      "Episode 240\tAvg Score: 3.78re: 3.78\n",
      "Episode 260\tAvg Score: 4.47re: 4.47\n",
      "Episode 280\tAvg Score: 5.05re: 5.05\n",
      "Episode 300\tAvg Score: 5.78re: 5.78\n",
      "Episode 320\tAvg Score: 6.25re: 6.25\n",
      "Episode 340\tAvg Score: 6.61re: 6.61\n",
      "Episode 360\tAvg Score: 7.11re: 7.11\n",
      "Episode 380\tAvg Score: 7.58re: 7.58\n",
      "Episode 400\tAvg Score: 7.62re: 7.62\n",
      "Episode 420\tAvg Score: 8.04re: 8.04\n",
      "Episode 440\tAvg Score: 8.09re: 8.09\n",
      "Episode 460\tAvg Score: 8.18re: 8.18\n",
      "Episode 480\tAvg Score: 8.23re: 8.23\n",
      "Episode 500\tAvg Score: 8.43re: 8.43\n",
      "Episode 520\tAvg Score: 8.47re: 8.47\n",
      "Episode 540\tAvg Score: 8.51re: 8.51\n",
      "Episode 560\tAvg Score: 8.43re: 8.43\n",
      "Episode 580\tAvg Score: 8.15re: 8.15\n",
      "Episode 600\tAvg Score: 7.79re: 7.79\n",
      "Episode 620\tAvg Score: 7.25re: 7.25\n",
      "Episode 640\tAvg Score: 7.13re: 7.13\n",
      "Episode 660\tAvg Score: 6.61re: 6.61\n",
      "Episode 680\tAvg Score: 6.44re: 6.44\n",
      "Episode 700\tAvg Score: 6.16re: 6.16\n",
      "Episode 720\tAvg Score: 5.91re: 5.91\n",
      "Episode 740\tAvg Score: 5.30re: 5.30\n",
      "Episode 760\tAvg Score: 4.86re: 4.86\n",
      "Episode 780\tAvg Score: 4.25re: 4.25\n",
      "Episode 800\tAvg Score: 3.97re: 3.97\n",
      "Episode 820\tAvg Score: 4.30re: 4.30\n",
      "Episode 840\tAvg Score: 4.43re: 4.43\n",
      "Episode 860\tAvg Score: 4.47re: 4.47\n",
      "Episode 880\tAvg Score: 4.82re: 4.82\n",
      "Episode 900\tAvg Score: 4.93re: 4.93\n",
      "Episode 920\tAvg Score: 4.70re: 4.70\n",
      "Episode 940\tAvg Score: 4.34re: 4.34\n",
      "Episode 960\tAvg Score: 4.62re: 4.62\n",
      "Episode 980\tAvg Score: 4.20re: 4.20\n",
      "Episode 1000\tAvg Score: 3.90re: 3.90\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not so good. Try reducing the theta - exploring too much?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0005\n",
      "lr_critic \t\t 0.0005\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 300\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.05\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run5\n",
      "save_data_name \t\t _data_default\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=300, save_name='run5', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 5e-4, lr_critic = 5e-4, oun_theta=0.05)\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.86e: 0.86\n",
      "Episode 40\tAvg Score: 1.02e: 1.02\n",
      "Episode 60\tAvg Score: 1.13e: 1.13\n",
      "Episode 80\tAvg Score: 1.33e: 1.33\n",
      "Episode 100\tAvg Score: 1.71e: 1.71\n",
      "Episode 120\tAvg Score: 2.42e: 2.42\n",
      "Episode 140\tAvg Score: 3.24e: 3.24\n",
      "Episode 160\tAvg Score: 4.54e: 4.54\n",
      "Episode 180\tAvg Score: 5.79e: 5.79\n",
      "Episode 200\tAvg Score: 7.19e: 7.19\n",
      "Episode 220\tAvg Score: 8.52e: 8.52\n",
      "Episode 240\tAvg Score: 10.06: 10.06\n",
      "Episode 260\tAvg Score: 11.11: 11.11\n",
      "Episode 280\tAvg Score: 12.44: 12.44\n",
      "Episode 300\tAvg Score: 13.09: 13.09\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores == ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training appears good. Yippee!. Continue training, but with loaded weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0005\n",
      "lr_critic \t\t 0.0005\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 1000\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.05\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run6\n",
      "save_data_name \t\t _data_default\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=1000, save_name='run6', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 5e-4, \n",
    "                                lr_critic = 5e-4, oun_theta=0.05)\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.39re: 0.39\n",
      "Episode 40\tAvg Score: 0.21re: 0.21\n",
      "Episode 60\tAvg Score: 0.23re: 0.23\n",
      "Episode 80\tAvg Score: 0.29re: 0.29\n",
      "Episode 100\tAvg Score: 0.38re: 0.38\n",
      "Episode 120\tAvg Score: 0.52re: 0.52\n",
      "Episode 140\tAvg Score: 0.82re: 0.82\n",
      "Episode 160\tAvg Score: 1.12re: 1.12\n",
      "Episode 180\tAvg Score: 1.41re: 1.41\n",
      "Episode 200\tAvg Score: 1.69re: 1.69\n",
      "Episode 220\tAvg Score: 1.86re: 1.86\n",
      "Episode 240\tAvg Score: 2.20re: 2.20\n",
      "Episode 260\tAvg Score: 2.51re: 2.51\n",
      "Episode 280\tAvg Score: 2.74re: 2.74\n",
      "Episode 300\tAvg Score: 3.10re: 3.10\n",
      "Episode 320\tAvg Score: 3.41re: 3.41\n",
      "Episode 340\tAvg Score: 3.64re: 3.64\n",
      "Episode 360\tAvg Score: 3.78re: 3.78\n",
      "Episode 380\tAvg Score: 3.99re: 3.99\n",
      "Episode 400\tAvg Score: 4.01re: 4.01\n",
      "Episode 420\tAvg Score: 4.17re: 4.17\n",
      "Episode 440\tAvg Score: 4.41re: 4.41\n",
      "Episode 460\tAvg Score: 4.74re: 4.74\n",
      "Episode 480\tAvg Score: 5.35re: 5.35\n",
      "Episode 500\tAvg Score: 5.99re: 5.99\n",
      "Episode 520\tAvg Score: 6.87re: 6.87\n",
      "Episode 540\tAvg Score: 7.40re: 7.40\n",
      "Episode 560\tAvg Score: 8.05re: 8.05\n",
      "Episode 580\tAvg Score: 8.43re: 8.43\n",
      "Episode 600\tAvg Score: 9.00re: 9.00\n",
      "Episode 620\tAvg Score: 9.32re: 9.32\n",
      "Episode 640\tAvg Score: 10.08e: 10.08\n",
      "Episode 660\tAvg Score: 10.72e: 10.72\n",
      "Episode 680\tAvg Score: 11.86e: 11.86\n",
      "Episode 700\tAvg Score: 12.87e: 12.87\n",
      "Episode 720\tAvg Score: 14.19e: 14.19\n",
      "Episode 740\tAvg Score: 15.99e: 15.99\n",
      "Episode 760\tAvg Score: 17.73e: 17.73\n",
      "Episode 780\tAvg Score: 19.05e: 19.05\n",
      "Episode 800\tAvg Score: 20.58e: 20.58\n",
      "Episode 820\tAvg Score: 21.83e: 21.83\n",
      "Episode 840\tAvg Score: 21.90e: 21.90\n",
      "Episode 860\tAvg Score: 22.81e: 22.81\n",
      "Episode 880\tAvg Score: 23.58e: 23.58\n",
      "Episode 900\tAvg Score: 23.60e: 23.60\n",
      "Episode 920\tAvg Score: 23.32e: 23.32\n",
      "Episode 940\tAvg Score: 23.84e: 23.84\n",
      "Episode 960\tAvg Score: 24.42e: 24.42\n",
      "Episode 980\tAvg Score: 24.88e: 24.88\n",
      "Episode 1000\tAvg Score: 26.20e: 26.20\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores_2 = ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those results looked good - just not enough time. I might tweak the learning rate slightly higher, but otherwise let it run for longer.  I also added code to pickle the data in stages so we can graph it, even if the connection with the server is lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0008\n",
      "lr_critic \t\t 0.0008\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 2000\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 20\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.05\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run7\n",
      "save_data_name \t\t run7_data_\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=2000, save_name='run7', print_every = 20, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 8e-4, \n",
    "                                lr_critic = 8e-4, oun_theta=0.05, save_data_name='run7_data_')\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tAvg Score: 0.11re: 0.11\n",
      "Episode 40\tAvg Score: 0.06re: 0.06\n",
      "Episode 60\tAvg Score: 0.13re: 0.13\n",
      "Episode 80\tAvg Score: 0.19re: 0.19\n",
      "Episode 100\tAvg Score: 0.24re: 0.24\n",
      "Episode 120\tAvg Score: 0.39re: 0.39\n",
      "Episode 140\tAvg Score: 0.53re: 0.53\n",
      "Episode 160\tAvg Score: 0.63re: 0.63\n",
      "Episode 180\tAvg Score: 0.66re: 0.66\n",
      "Episode 200\tAvg Score: 0.72re: 0.72\n",
      "Episode 220\tAvg Score: 0.68re: 0.68\n",
      "Episode 240\tAvg Score: 0.69re: 0.69\n",
      "Episode 260\tAvg Score: 0.70re: 0.70\n",
      "Episode 280\tAvg Score: 0.73re: 0.73\n",
      "Episode 300\tAvg Score: 0.69re: 0.69\n",
      "Episode 320\tAvg Score: 0.76re: 0.76\n",
      "Episode 340\tAvg Score: 0.78re: 0.78\n",
      "Episode 360\tAvg Score: 0.83re: 0.83\n",
      "Episode 380\tAvg Score: 0.86re: 0.86\n",
      "Episode 400\tAvg Score: 0.98re: 0.98\n",
      "Episode 420\tAvg Score: 0.98re: 0.98\n",
      "Episode 440\tAvg Score: 0.99re: 0.99\n",
      "Episode 460\tAvg Score: 0.96re: 0.96\n",
      "Episode 480\tAvg Score: 1.01re: 1.01\n",
      "Episode 500\tAvg Score: 0.97re: 0.97\n",
      "Episode 520\tAvg Score: 0.99re: 0.99\n",
      "Episode 540\tAvg Score: 0.90re: 0.90\n",
      "Episode 560\tAvg Score: 0.87re: 0.87\n",
      "Episode 580\tAvg Score: 0.90re: 0.90\n",
      "Episode 600\tAvg Score: 0.91re: 0.91\n",
      "Episode 620\tAvg Score: 0.86re: 0.86\n",
      "Episode 640\tAvg Score: 0.99re: 0.99\n",
      "Episode 660\tAvg Score: 1.02re: 1.02\n",
      "Episode 680\tAvg Score: 0.96re: 0.96\n",
      "Episode 700\tAvg Score: 0.92re: 0.92\n",
      "Episode 720\tAvg Score: 0.85re: 0.85\n",
      "Episode 740\tAvg Score: 0.61re: 0.61\n",
      "Episode 760\tAvg Score: 0.47re: 0.47\n",
      "Episode 780\tAvg Score: 0.43re: 0.43\n",
      "Episode 800\tAvg Score: 0.44re: 0.44\n",
      "Episode 820\tAvg Score: 0.50re: 0.50\n",
      "Episode 840\tAvg Score: 0.65re: 0.65\n",
      "Episode 860\tAvg Score: 0.79re: 0.79\n",
      "Episode 880\tAvg Score: 0.81re: 0.81\n",
      "Episode 900\tAvg Score: 0.80re: 0.80\n",
      "Episode 920\tAvg Score: 0.82re: 0.82\n",
      "Episode 940\tAvg Score: 0.84re: 0.84\n",
      "Episode 960\tAvg Score: 0.82re: 0.82\n",
      "Episode 980\tAvg Score: 0.85re: 0.85\n",
      "Episode 1000\tAvg Score: 0.85re: 0.85\n",
      "Episode 1020\tAvg Score: 0.86re: 0.86\n",
      "Episode 1040\tAvg Score: 0.87re: 0.87\n",
      "Episode 1060\tAvg Score: 0.94re: 0.94\n",
      "Episode 1080\tAvg Score: 0.90re: 0.90\n",
      "Episode 1100\tAvg Score: 0.93re: 0.93\n",
      "Episode 1120\tAvg Score: 0.95re: 0.95\n",
      "Episode 1140\tAvg Score: 0.92re: 0.92\n",
      "Episode 1160\tAvg Score: 0.88re: 0.88\n",
      "Episode 1180\tAvg Score: 0.88re: 0.88\n",
      "Episode 1200\tAvg Score: 0.90re: 0.90\n",
      "Episode 1220\tAvg Score: 0.89re: 0.89\n",
      "Episode 1240\tAvg Score: 0.88re: 0.88\n",
      "Episode 1260\tAvg Score: 0.87re: 0.87\n",
      "Episode 1280\tAvg Score: 0.87re: 0.87\n",
      "Episode 1300\tAvg Score: 0.84re: 0.84\n",
      "Episode 1320\tAvg Score: 0.85re: 0.85\n",
      "Episode 1340\tAvg Score: 0.87re: 0.87\n",
      "Episode 1360\tAvg Score: 0.83re: 0.83\n",
      "Episode 1380\tAvg Score: 0.86re: 0.86\n",
      "Episode 1400\tAvg Score: 0.85re: 0.85\n",
      "Episode 1420\tAvg Score: 0.85re: 0.85\n",
      "Episode 1440\tAvg Score: 0.87re: 0.87\n",
      "Episode 1460\tAvg Score: 0.90re: 0.90\n",
      "Episode 1480\tAvg Score: 0.90re: 0.90\n",
      "Episode 1500\tAvg Score: 0.87re: 0.87\n",
      "Episode 1520\tAvg Score: 0.86re: 0.86\n",
      "Episode 1540\tAvg Score: 0.82re: 0.82\n",
      "Episode 1560\tAvg Score: 0.81re: 0.81\n",
      "Episode 1580\tAvg Score: 0.78re: 0.78\n",
      "Episode 1600\tAvg Score: 0.77re: 0.77\n",
      "Episode 1620\tAvg Score: 0.81re: 0.81\n",
      "Episode 1640\tAvg Score: 0.83re: 0.83\n",
      "Episode 1660\tAvg Score: 0.87re: 0.87\n",
      "Episode 1680\tAvg Score: 0.87re: 0.87\n",
      "Episode 1700\tAvg Score: 0.95re: 0.95\n",
      "Episode 1720\tAvg Score: 0.94re: 0.94\n",
      "Episode 1740\tAvg Score: 0.92re: 0.92\n",
      "Episode 1760\tAvg Score: 0.88re: 0.88\n",
      "Episode 1780\tAvg Score: 0.91re: 0.91\n",
      "Episode 1800\tAvg Score: 0.84re: 0.84\n",
      "Episode 1820\tAvg Score: 0.83re: 0.83\n",
      "Episode 1840\tAvg Score: 0.87re: 0.87\n",
      "Episode 1860\tAvg Score: 0.91re: 0.91\n",
      "Episode 1880\tAvg Score: 0.92re: 0.92\n",
      "Episode 1900\tAvg Score: 0.92re: 0.92\n",
      "Episode 1920\tAvg Score: 0.94re: 0.94\n",
      "Episode 1940\tAvg Score: 0.90re: 0.90\n",
      "Episode 1960\tAvg Score: 0.89re: 0.89\n",
      "Episode 1980\tAvg Score: 0.86re: 0.86\n",
      "Episode 2000\tAvg Score: 0.84re: 0.84\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores = ddpg(hypers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did not work - revert the learning rate back down (even lower). And re-try...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0003\n",
      "lr_critic \t\t 0.0003\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 2000\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 50\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.05\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run8\n",
      "save_data_name \t\t run8_data_\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=2000, save_name='run8', print_every = 50, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 3e-4, \n",
    "                                lr_critic = 3e-4, oun_theta=0.05, save_data_name='run8_data_')\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAvg Score: 0.60re: 0.60\n",
      "Episode 100\tAvg Score: 0.93re: 0.93\n",
      "Episode 150\tAvg Score: 2.27re: 2.27\n",
      "Episode 200\tAvg Score: 5.19re: 5.19\n",
      "Episode 250\tAvg Score: 10.22e: 10.22\n",
      "Episode 300\tAvg Score: 17.37e: 17.37\n",
      "Episode 350\tAvg Score: 25.14e: 25.14\n",
      "Episode 380 of 2000\tAvg Score: 30.21Task completed on episode 380\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores = ddpg(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb4434f6668>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb45026ce48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.savefig('solved.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was a bit easy - can see if I can get a higher score now? Changed the code in ddpg to get a score > 40 as a success measure.  \n",
    "The *only* change in the ddpg code below is the use of a score-target now, instead of hard-coded figure of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg(hypers, score_target=30):\n",
    "    \"\"\"Run the DDPG agent through episodes, using Actor and Critic\"\"\"\n",
    "    \n",
    "    agent = Agent(hypers=hypers)\n",
    "    if hypers['load_from_actor']:\n",
    "        agent.actor_local.load_state_dict(hypers['load_from_actor'])\n",
    "        \n",
    "    if hypers['load_from_critic']:\n",
    "        agent.critic_local.load_state_dict(hypers['load_from_critic'])\n",
    "    \n",
    "    n_episodes = hypers['n_episodes']\n",
    "    max_t = hypers['max_t']\n",
    "    print_every = hypers['print_every']\n",
    "    save_every = hypers['save_every']\n",
    "    check_point_actor = 'checkpoint_actor_' + hypers['save_name']+'_'\n",
    "    check_point_critic = 'checkpoint_critic_' + hypers['save_name']+'_'\n",
    "    scores_deque = deque(maxlen = 100) # this is the test we are using - have to get avg > 30 over 100 episodes\n",
    "    scores=[]\n",
    "    epsilon = 1\n",
    "    for i_episode in range(1, n_episodes +1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state =env_info.vector_observations\n",
    "        agent.reset() # note - this resets the noise only!\n",
    "#         epsilon *= EPSILON_RATE\n",
    "        \n",
    "        score=0\n",
    "        while True:\n",
    "#         for t in range(max_t):\n",
    "            action = agent.act(state, epsilon)\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            next_state = env_info.vector_observations\n",
    "            reward = env_info.rewards\n",
    "            done = env_info.local_done\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward[0]\n",
    "            if done[0]:\n",
    "                break\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {} of {}\\tAvg Score: {:.2f}'.format(i_episode, n_episodes, np.mean(scores_deque)), end=\"\")\n",
    "        if i_episode % save_every == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), check_point_actor + str(i_episode))\n",
    "            torch.save(agent.critic_local.state_dict(), check_point_critic + str(i_episode))\n",
    "            with open(hypers['save_data_name']+'.pkl', 'wb') as file:\n",
    "                pickle.dump(scores, file)\n",
    "        if i_episode % print_every == 0:\n",
    "                print('\\rEpisode {}\\tAvg Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        if np.mean(scores_deque) > score_target:\n",
    "            print ('Task completed on episode {}'.format(i_episode))\n",
    "            torch.save(agent.actor_local.state_dict(), check_point_actor + '_SOLVED')\n",
    "            torch.save(agent.critic_local.state_dict(), check_point_critic + '_SOLVED')\n",
    "            with open(hypers['save_data_name']+'.pkl', 'wb') as file:\n",
    "                pickle.dump(scores, file)\n",
    "            break\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyper-parameters used\n",
      "state_size \t\t 33\n",
      "action_size \t\t 4\n",
      "random_seed \t\t 101\n",
      "buffer_size \t\t 100000\n",
      "batch_size \t\t 128\n",
      "gamma \t\t\t 0.99\n",
      "tau \t\t\t 0.001\n",
      "lr_actor \t\t 0.0003\n",
      "lr_critic \t\t 0.0003\n",
      "weight_decay \t\t 0\n",
      "learn_every \t\t 5\n",
      "n_episodes \t\t 2000\n",
      "max_t \t\t\t 1000\n",
      "print_every \t\t 50\n",
      "oun_mu \t\t\t 0.0\n",
      "oun_theta \t\t 0.05\n",
      "oun_sigma \t\t 0.2\n",
      "actor_fc1_units \t 256\n",
      "actor_fc2_units \t 128\n",
      "critic_fcs1_units \t 256\n",
      "critic_fc2_units \t 128\n",
      "save_every \t\t 20\n",
      "save_name \t\t run9\n",
      "save_data_name \t\t run8_data_\n",
      "load_from_actor \t \n",
      "load_from_critic \t \n"
     ]
    }
   ],
   "source": [
    "hypers = set_up_hyperparameters(n_episodes=2000, save_name='run9', print_every = 50, actor_fc1_units=256,\n",
    "                               critic_fcs1_units=256, batch_size=128, lr_actor = 3e-4, \n",
    "                                lr_critic = 3e-4, oun_theta=0.05, save_data_name='run8_data_')\n",
    "pprint(hypers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50\tAvg Score: 0.63re: 0.63\n",
      "Episode 100\tAvg Score: 1.10re: 1.10\n",
      "Episode 150\tAvg Score: 2.13re: 2.13\n",
      "Episode 200\tAvg Score: 3.77re: 3.77\n",
      "Episode 250\tAvg Score: 6.67re: 6.67\n",
      "Episode 300\tAvg Score: 9.06re: 9.06\n",
      "Episode 350\tAvg Score: 10.46e: 10.46\n",
      "Episode 400\tAvg Score: 10.96e: 10.96\n",
      "Episode 450\tAvg Score: 10.84e: 10.84\n",
      "Episode 500\tAvg Score: 11.25e: 11.25\n",
      "Episode 550\tAvg Score: 11.58e: 11.58\n",
      "Episode 600\tAvg Score: 10.75e: 10.75\n",
      "Episode 650\tAvg Score: 10.22e: 10.22\n",
      "Episode 700\tAvg Score: 10.64e: 10.64\n",
      "Episode 750\tAvg Score: 11.32e: 11.32\n",
      "Episode 800\tAvg Score: 11.95e: 11.95\n",
      "Episode 850\tAvg Score: 12.16e: 12.16\n",
      "Episode 900\tAvg Score: 13.09e: 13.09\n",
      "Episode 950\tAvg Score: 13.78e: 13.78\n",
      "Episode 1000\tAvg Score: 14.35e: 14.35\n",
      "Episode 1050\tAvg Score: 15.17e: 15.17\n",
      "Episode 1100\tAvg Score: 16.06e: 16.06\n",
      "Episode 1150\tAvg Score: 16.70e: 16.70\n",
      "Episode 1200\tAvg Score: 16.97e: 16.97\n",
      "Episode 1250\tAvg Score: 19.36e: 19.36\n",
      "Episode 1300\tAvg Score: 22.38e: 22.38\n",
      "Episode 1350\tAvg Score: 25.33e: 25.33\n",
      "Episode 1400\tAvg Score: 28.36e: 28.36\n",
      "Episode 1450\tAvg Score: 29.51e: 29.51\n",
      "Episode 1500\tAvg Score: 30.31e: 30.31\n",
      "Episode 1550\tAvg Score: 32.67e: 32.67\n",
      "Episode 1600\tAvg Score: 32.16e: 32.16\n",
      "Episode 1650\tAvg Score: 28.82e: 28.82\n",
      "Episode 1700\tAvg Score: 28.49e: 28.49\n",
      "Episode 1750\tAvg Score: 30.45e: 30.45\n",
      "Episode 1800\tAvg Score: 31.79e: 31.79\n",
      "Episode 1850\tAvg Score: 31.47e: 31.47\n",
      "Episode 1900\tAvg Score: 29.89e: 29.89\n",
      "Episode 1950\tAvg Score: 28.65e: 28.65\n",
      "Episode 2000\tAvg Score: 28.48e: 28.48\n"
     ]
    }
   ],
   "source": [
    "with active_session():\n",
    "    scores = ddpg(hypers, score_target=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
